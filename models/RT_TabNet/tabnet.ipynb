{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/louan/Documents/projects/Aiffel/Hackathon/geodata-con/geo-con/model/tabnet'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler # 표준화 패키지 라이브러리\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from warnings import filterwarnings\n",
    "import gc\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filterwarnings('ignore')\n",
    "device = torch.device('cpu')\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../data/pre_processed_split_data_0.pkl', '../../../data/pre_processed_split_data_1.pkl', '../../../data/pre_processed_split_data_2.pkl', '../../../data/pre_processed_split_data_3.pkl', '../../../data/pre_processed_split_data_4.pkl', '../../../data/pre_processed_split_data_5.pkl', '../../../data/pre_processed_split_0tpt_data_6.pkl', '../../../data/pre_processed_split_data_7.pkl', '../../../data/pre_processed_split_data_8.pkl']\n"
     ]
    }
   ],
   "source": [
    "path = '../../../data/*.pkl'\n",
    "\n",
    "file_lst = sorted(glob(path))[1:]\n",
    "file_lst[6] = '../../../data/pre_processed_split_0tpt_data_6.pkl'\n",
    "print(file_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ../../../data/pre_processed_split_data_0.pkl\n",
      "1 ../../../data/pre_processed_split_data_1.pkl\n",
      "2 ../../../data/pre_processed_split_data_2.pkl\n",
      "3 ../../../data/pre_processed_split_data_3.pkl\n",
      "4 ../../../data/pre_processed_split_data_4.pkl\n",
      "5 ../../../data/pre_processed_split_data_5.pkl\n",
      "6 ../../../data/pre_processed_split_0tpt_data_6.pkl\n",
      "7 ../../../data/pre_processed_split_data_7.pkl\n",
      "8 ../../../data/pre_processed_split_data_8.pkl\n"
     ]
    }
   ],
   "source": [
    "# ModuleNotFoundError: No module named 'pandas.core.indexes.numeric' using Metaflow\n",
    "# This issue is caused by the new Pandas 2.0.0 release => pip install \"pandas<2.0.0\"\n",
    "# https://stackoverflow.com/questions/75953279/modulenotfounderror-no-module-named-pandas-core-indexes-numeric-using-metaflo\n",
    "for i,path in enumerate(file_lst):\n",
    "    print(i, path)\n",
    "    with open(path, 'rb') as f:\n",
    "        globals()['df_event_{}'.format(i)] = pickle.load(f)\n",
    "        globals()['df_event_{}'.format(i)].reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier 제거\n",
    "- 참고: https://stackoverflow.com/questions/35827863/remove-outliers-in-pandas-dataframe-using-percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add delta and remove outlier\n",
    "def add_delta(data_frame):\n",
    "    for column in ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']:\n",
    "        column_pctchg = column+'_pctchg'\n",
    "        data_frame[column_pctchg] = data_frame.groupby(['id_label'])[column].pct_change()\n",
    "        data_frame[column_pctchg][data_frame[column_pctchg].isna()] = 0\n",
    "    return data_frame\n",
    "\n",
    "def remove_outlier(df):\n",
    "    target_cols = ['P-PDG', 'P-TPT', 'T-TPT','P-MON-CKP', 'T-JUS-CKP',]\n",
    "    df = df[df['id_label'].str.startswith('WELL')]\n",
    "    df = add_delta(df)\n",
    "    Q1 = df[target_cols].quantile(0.25)\n",
    "    Q3 = df[target_cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df[target_cols] < (Q1 - 1.5 * IQR)) | (df[target_cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    print(i)\n",
    "    df_event_0_out = remove_outlier(df_event_0)\n",
    "    df_event_1_out = remove_outlier(df_event_1)\n",
    "    df_event_2_out = remove_outlier(df_event_2)\n",
    "    df_event_3_out = remove_outlier(df_event_3)\n",
    "    df_event_4_out = remove_outlier(df_event_4)\n",
    "    df_event_5_out = remove_outlier(df_event_5)\n",
    "    df_event_6_out = remove_outlier(df_event_6)\n",
    "    df_event_7_out = remove_outlier(df_event_7)\n",
    "    df_event_8_out = remove_outlier(df_event_8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the list of each data type(WELL, SIMULATED, DRAWN)\n",
    "instance_dict = {'WELL': [], 'SIMULATED': [], 'DRAWN': []}\n",
    "for i in range(9):\n",
    "  instance_lst = list(globals()['df_event_{}'.format(i)]['id_label'].unique())\n",
    "  for instance in instance_lst:\n",
    "    if instance.startswith('WELL'):\n",
    "      instance_dict['WELL'].extend([instance])\n",
    "    elif instance.startswith('SIMULATED'):\n",
    "      instance_dict['SIMULATED'].extend([instance])\n",
    "    elif instance.startswith('DRAWN'):\n",
    "      instance_dict['DRAWN'].extend([instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 511, '4': 344, '3': 32, '6': 6, '7': 5, '1': 4, '2': 3, '5': 3})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of events in 'WELL' data\n",
    "class_lst = []\n",
    "for i in instance_dict['WELL']:\n",
    "  class_lst.append(i.split('_')[-1])\n",
    "Counter(class_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test instances\n",
    "def split_train_test_instance(instance_dict, dTypes = ['WELL', 'SIMULATED', 'DRAWN'],train_ratio = 0.6, test_ratio = 0.2):\n",
    "    total_len = 0\n",
    "    simul_drawn_lst = []\n",
    "    for dType in dTypes:\n",
    "        total_len += len(instance_dict[dType])\n",
    "        simul_drawn_lst += instance_dict[dType]\n",
    "    well_len = len(instance_dict['WELL'])\n",
    "\n",
    "    num_of_test = int(total_len*test_ratio)\n",
    "    num_of_val = int(total_len*(1-train_ratio-test_ratio))\n",
    "    num_of_train = total_len - num_of_test - num_of_val\n",
    "\n",
    "    # extract test_lst first(Only WELL data)\n",
    "    test_lst = random.sample(instance_dict['WELL'], num_of_test)\n",
    "    \n",
    "    # then make whole lst --> shuffle lst --> split train validation set\n",
    "    rest_well_lst = [i for i in instance_dict['WELL'] if i not in test_lst]\n",
    "    total_lst = rest_well_lst + simul_drawn_lst\n",
    "    random.shuffle(total_lst)\n",
    "\n",
    "    train_lst = total_lst[:num_of_train]\n",
    "    val_lst = total_lst[num_of_train:]\n",
    "\n",
    "    return train_lst, val_lst, test_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1307\n",
      "1188\n",
      "280\n",
      "total number of lst: 1867\n",
      "total number of lst after removing duplicated: 1867\n",
      "train:\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
      "val:\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
      "test:\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "val_ratio = 1 - train_ratio - test_ratio\n",
    "dTypes = ['WELL', 'SIMULATED', 'DRAWN']\n",
    "train_lst, val_lst, test_lst = split_train_test_instance(instance_dict, dTypes = dTypes, train_ratio = train_ratio, test_ratio = test_ratio)\n",
    "\n",
    "print(len(train_lst), len(val_lst), len(test_lst), sep = '\\n')\n",
    "print('total number of lst:', len(instance_dict['WELL'])+len(instance_dict['DRAWN'])+len(instance_dict['SIMULATED']))\n",
    "print('total number of lst after removing duplicated:', len(set(train_lst + val_lst + test_lst)))\n",
    "print('train:', sorted(set([i.split('_')[-1] for i in train_lst])), 'val:', sorted(set([i.split('_')[-1] for i in val_lst])), 'test:', sorted(set([i.split('_')[-1] for i in test_lst])), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: https://rfriend.tistory.com/590\n",
    "def make_empty_df(pca_n_components):\n",
    "    pca_lst = ['pca_1', 'pca_2','pca_3', 'pca_4', 'pca_5']\n",
    "    columns = ['P-PDG', 'P-PDG_std','P-PDG_pctchg_mean', 'P-PDG_pctchg_std',\n",
    "               'P-TPT', 'P-TPT_std', 'P-TPT_pctchg_mean', 'P-TPT_pctchg_std',\n",
    "               'T-TPT', 'T-TPT_std', 'T-TPT_pctchg_mean', 'T-TPT_pctchg_std',\n",
    "               'P-MON-CKP', 'P-MON-CKP_std', 'P-MON-CKP_pctchg_mean', 'P-MON-CKP_pctchg_std',\n",
    "               'T-JUS-CKP', 'T-JUS-CKP_std', 'T-JUS-CKP_pctchg_mean', 'T-JUS-CKP_pctchg_std',\n",
    "               'class',]\n",
    "    columns.extend(pca_lst[:pca_n_components])\n",
    "    data_frame = pd.DataFrame(columns=columns)\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def apply_window(data_frame, window_size = 5000, overlap_ratio = 0.2, pca_n_components = 2):\n",
    "    if pca_n_components:\n",
    "        pca = PCA(n_components=pca_n_components)\n",
    "    \n",
    "    new_df = make_empty_df(pca_n_components)\n",
    "    for n in tqdm(range(int(len(data_frame)/(window_size*(1-overlap_ratio))))):\n",
    "        try:\n",
    "            if int(window_size*(n+1) - window_size*overlap_ratio*n) <= len(data_frame):\n",
    "                # print(window_size*n*(1-overlap_ratio),window_size*(n+1) - window_size*overlap_ratio*n)\n",
    "                sliced_df = data_frame[int(window_size*n*(1-overlap_ratio)):int(window_size*(n+1) - window_size*overlap_ratio*n)]\n",
    "                pca_data = pca.fit_transform(sliced_df.drop( ['id_label','class'], axis=1))\n",
    "                if any(sliced_df['class'] != 0):\n",
    "                    class_label =  int(sliced_df['class'].value_counts().index[sliced_df['class'].value_counts().index < 10][0]) # 기본 모드\n",
    "                    \n",
    "                    # class_label = 1\n",
    "                else:\n",
    "                    # continue\n",
    "                    class_label = 0\n",
    "                new_df = pd.concat([new_df,pd.DataFrame({'P-PDG':[sliced_df['P-PDG'].mean()],\n",
    "                                                        'P-PDG_std':[sliced_df['P-PDG'].std()],\n",
    "                                                        'P-PDG_pctchg_mean':[sliced_df['P-PDG_pctchg'].mean()],\n",
    "                                                        'P-PDG_pctchg_std':[sliced_df['P-PDG_pctchg'].std()],\n",
    "                                                        'P-TPT':[sliced_df['P-TPT'].mean()],\n",
    "                                                        'P-TPT_std':[sliced_df['P-TPT'].std()],\n",
    "                                                        'P-TPT_pctchg_mean':[sliced_df['P-TPT_pctchg'].mean()],\n",
    "                                                        'P-TPT_pctchg_std':[sliced_df['P-TPT_pctchg'].std()],\n",
    "                                                        'T-TPT':[sliced_df['T-TPT'].mean()],\n",
    "                                                        'T-TPT_std':[sliced_df['T-TPT'].std()],\n",
    "                                                        'T-TPT_pctchg_mean':[sliced_df['T-TPT_pctchg'].mean()],\n",
    "                                                        'T-TPT_pctchg_std':[sliced_df['T-TPT_pctchg'].std()],\n",
    "                                                        'P-MON-CKP':[sliced_df['P-MON-CKP'].mean()],\n",
    "                                                        'P-MON-CKP_std':[sliced_df['P-MON-CKP'].std()],\n",
    "                                                        'P-MON-CKP_pctchg_mean':[sliced_df['P-MON-CKP_pctchg'].mean()],\n",
    "                                                        'P-MON-CKP_pctchg_std':[sliced_df['P-MON-CKP_pctchg'].std()],\n",
    "                                                        'T-JUS-CKP':[sliced_df['T-JUS-CKP'].mean()],\n",
    "                                                        'T-JUS-CKP_std':[sliced_df['T-JUS-CKP'].std()],\n",
    "                                                        'T-JUS-CKP_pctchg_mean':[sliced_df['T-JUS-CKP_pctchg'].mean()],\n",
    "                                                        'T-JUS-CKP_pctchg_std':[sliced_df['T-JUS-CKP_pctchg'].std()],\n",
    "                                                        'class': class_label,\n",
    "                                                        'pca_1':[pca_data[:,0].mean()],\n",
    "                                                        'pca_2':[pca_data[:,1].mean()],\n",
    "                                                        #'pca_3':[pca_data.iloc[:,2].mean()],\n",
    "                                                        })])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    return new_df\n",
    "\n",
    "def build_dataset(data_frame, train_lst, val_lst, test_lst, window_size = 5000, overlap_ratio = 0.2, pca_n_components = 2):\n",
    "  gc.collect()\n",
    "  train_data_frame = data_frame[data_frame['id_label'].isin(train_lst)]\n",
    "  val_data_frame = data_frame[data_frame['id_label'].isin(val_lst)]\n",
    "  test_data_frame = data_frame[data_frame['id_label'].isin(test_lst)]\n",
    "\n",
    "#   train_data_frame = remove_outlier(train_data_frame)\n",
    "\n",
    "  train_df = apply_window(train_data_frame, window_size = window_size, overlap_ratio = overlap_ratio)\n",
    "  val_df = apply_window(val_data_frame, window_size = window_size, overlap_ratio = overlap_ratio)\n",
    "  test_df = apply_window(test_data_frame, window_size = window_size, overlap_ratio = overlap_ratio)\n",
    "  return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64036/64036 [08:21<00:00, 127.81it/s]\n",
      "100%|██████████| 53541/53541 [06:05<00:00, 146.65it/s]\n",
      "100%|██████████| 27949/27949 [03:03<00:00, 152.72it/s]\n",
      "100%|██████████| 750/750 [00:04<00:00, 164.68it/s]\n",
      "100%|██████████| 2493/2493 [00:13<00:00, 184.33it/s]\n",
      "100%|██████████| 3244/3244 [00:17<00:00, 189.33it/s]\n",
      "100%|██████████| 275/275 [00:02<00:00, 134.27it/s]\n",
      "100%|██████████| 89/89 [00:00<00:00, 182.08it/s]\n",
      "100%|██████████| 112/112 [00:00<00:00, 184.79it/s]\n",
      "100%|██████████| 5677/5677 [00:36<00:00, 155.23it/s]\n",
      "100%|██████████| 6488/6488 [00:40<00:00, 159.82it/s]\n",
      "100%|██████████| 1588/1588 [00:10<00:00, 157.60it/s]\n",
      "100%|██████████| 23050/23050 [02:56<00:00, 130.53it/s]\n",
      "100%|██████████| 22329/22329 [02:27<00:00, 151.74it/s]\n",
      "100%|██████████| 8938/8938 [00:58<00:00, 153.17it/s]\n",
      "100%|██████████| 3921/3921 [00:16<00:00, 240.04it/s]\n",
      "100%|██████████| 2417/2417 [00:09<00:00, 246.22it/s]\n",
      "100%|██████████| 1504/1504 [00:07<00:00, 200.41it/s]\n",
      "100%|██████████| 600/600 [00:03<00:00, 171.45it/s]\n",
      "100%|██████████| 385/385 [00:02<00:00, 167.22it/s]\n",
      "100%|██████████| 191/191 [00:01<00:00, 177.51it/s]\n",
      "100%|██████████| 4390/4390 [00:20<00:00, 209.54it/s]\n",
      "100%|██████████| 4512/4512 [00:20<00:00, 222.14it/s]\n",
      "100%|██████████| 1339/1339 [00:06<00:00, 212.83it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying window\n",
    "window_size = 1200\n",
    "window_size_short = window_size\n",
    "window_size_long =  window_size\n",
    "pca_n_components = 2\n",
    "overlap_ratio = 0.95\n",
    "\n",
    "df_event_0_window_train, df_event_0_window_val, df_event_0_window_test = build_dataset(df_event_0_out, train_lst, val_lst, test_lst, window_size = window_size_long, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_1_window_train, df_event_1_window_val, df_event_1_window_test = build_dataset(df_event_1_out, train_lst, val_lst, test_lst, window_size = window_size_long, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_2_window_train, df_event_2_window_val, df_event_2_window_test = build_dataset(df_event_2_out, train_lst, val_lst, test_lst, window_size = window_size_short, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_3_window_train, df_event_3_window_val, df_event_3_window_test = build_dataset(df_event_3_out, train_lst, val_lst, test_lst, window_size = window_size_long, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_4_window_train, df_event_4_window_val, df_event_4_window_test = build_dataset(df_event_4_out, train_lst, val_lst, test_lst, window_size = window_size_short, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_5_window_train, df_event_5_window_val, df_event_5_window_test = build_dataset(df_event_5_out, train_lst, val_lst, test_lst, window_size = window_size_long, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_6_window_train, df_event_6_window_val, df_event_6_window_test = build_dataset(df_event_6_out, train_lst, val_lst, test_lst, window_size = window_size_short, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_7_window_train, df_event_7_window_val, df_event_7_window_test = build_dataset(df_event_7_out, train_lst, val_lst, test_lst, window_size = window_size_long, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_8_window_train, df_event_8_window_val, df_event_8_window_test = build_dataset(df_event_8_out, train_lst, val_lst, test_lst, window_size = window_size_short, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling for train dataset\n",
    "merged_df_train =  pd.concat([df_event_0_window_train.sample(int(len(df_event_0_window_train)*0.1)),\n",
    "                              df_event_1_window_train,\n",
    "                              df_event_2_window_train,\n",
    "                              df_event_3_window_train.sample(int(len(df_event_3_window_train)*0.1)),\n",
    "                              df_event_4_window_train.sample(int(len(df_event_4_window_train)*0.1)),\n",
    "                              df_event_5_window_train,\n",
    "                              df_event_6_window_train,\n",
    "                              df_event_7_window_train,\n",
    "                              df_event_8_window_train])\n",
    "\n",
    "merged_df_val =  pd.concat([df_event_0_window_val,\n",
    "                            df_event_1_window_val,\n",
    "                            df_event_2_window_val,\n",
    "                            df_event_3_window_val,\n",
    "                            df_event_4_window_val,\n",
    "                            df_event_5_window_val,\n",
    "                            df_event_6_window_val,\n",
    "                            df_event_7_window_val,\n",
    "                            df_event_8_window_val])\n",
    "\n",
    "merged_df_test =  pd.concat([df_event_0_window_test,\n",
    "                             df_event_1_window_test,\n",
    "                             df_event_2_window_test,\n",
    "                             df_event_3_window_test,\n",
    "                             df_event_4_window_test,\n",
    "                             df_event_5_window_test,\n",
    "                             df_event_6_window_test,\n",
    "                             df_event_7_window_test,\n",
    "                             df_event_8_window_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "0    10070\n",
      "4     2301\n",
      "3      565\n",
      "6      198\n",
      "5       70\n",
      "7       64\n",
      "2       58\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    56869\n",
      "4    22300\n",
      "3     6469\n",
      "6       96\n",
      "7       59\n",
      "5       30\n",
      "2        3\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    31604\n",
      "4     8919\n",
      "3     1569\n",
      "5       30\n",
      "2       19\n",
      "6       10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df_train['class'].value_counts())\n",
    "print(merged_df_val['class'].value_counts())\n",
    "print(merged_df_test['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-PDG_pctchg_mean = -inf, 혹은 inf인 row 제외: train(14개), val(10개), test(13)\n",
    "merged_df_train = merged_df_train[~merged_df_train['P-PDG_pctchg_std'].isna()]\n",
    "merged_df_val = merged_df_val[~merged_df_val['P-PDG_pctchg_std'].isna()]\n",
    "merged_df_test = merged_df_test[~merged_df_test['P-PDG_pctchg_std'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P-PDG</th>\n",
       "      <th>P-PDG_std</th>\n",
       "      <th>P-PDG_pctchg_mean</th>\n",
       "      <th>P-PDG_pctchg_std</th>\n",
       "      <th>P-TPT</th>\n",
       "      <th>P-TPT_std</th>\n",
       "      <th>P-TPT_pctchg_mean</th>\n",
       "      <th>P-TPT_pctchg_std</th>\n",
       "      <th>T-TPT</th>\n",
       "      <th>T-TPT_std</th>\n",
       "      <th>...</th>\n",
       "      <th>P-MON-CKP</th>\n",
       "      <th>P-MON-CKP_std</th>\n",
       "      <th>P-MON-CKP_pctchg_mean</th>\n",
       "      <th>P-MON-CKP_pctchg_std</th>\n",
       "      <th>T-JUS-CKP</th>\n",
       "      <th>T-JUS-CKP_std</th>\n",
       "      <th>T-JUS-CKP_pctchg_mean</th>\n",
       "      <th>T-JUS-CKP_pctchg_std</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.565903e+06</td>\n",
       "      <td>61056.170215</td>\n",
       "      <td>2.078730e-05</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>118.041962</td>\n",
       "      <td>0.222718</td>\n",
       "      <td>...</td>\n",
       "      <td>1.749214e+06</td>\n",
       "      <td>236860.281168</td>\n",
       "      <td>-3.093040e-05</td>\n",
       "      <td>0.008582</td>\n",
       "      <td>75.105912</td>\n",
       "      <td>0.472400</td>\n",
       "      <td>-2.244456e-06</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>-3.414849e-11</td>\n",
       "      <td>8.723388e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.419693e+06</td>\n",
       "      <td>4233.729173</td>\n",
       "      <td>-1.972196e-06</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>117.029977</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>...</td>\n",
       "      <td>1.667997e+06</td>\n",
       "      <td>230222.740329</td>\n",
       "      <td>5.592193e-05</td>\n",
       "      <td>0.010094</td>\n",
       "      <td>74.859587</td>\n",
       "      <td>0.356903</td>\n",
       "      <td>-6.398129e-06</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>-3.104409e-12</td>\n",
       "      <td>5.217347e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.748236e+06</td>\n",
       "      <td>15403.806555</td>\n",
       "      <td>2.078632e-06</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>117.459051</td>\n",
       "      <td>0.047106</td>\n",
       "      <td>...</td>\n",
       "      <td>1.759294e+06</td>\n",
       "      <td>196850.107693</td>\n",
       "      <td>6.716495e-05</td>\n",
       "      <td>0.008226</td>\n",
       "      <td>75.750729</td>\n",
       "      <td>0.587198</td>\n",
       "      <td>5.073188e-07</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>1.008933e-10</td>\n",
       "      <td>-5.122274e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.828215e+06</td>\n",
       "      <td>10590.417050</td>\n",
       "      <td>-1.189768e-06</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>117.624216</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>...</td>\n",
       "      <td>1.759798e+06</td>\n",
       "      <td>200198.675728</td>\n",
       "      <td>-2.761304e-05</td>\n",
       "      <td>0.008616</td>\n",
       "      <td>77.141361</td>\n",
       "      <td>0.478572</td>\n",
       "      <td>-3.179340e-06</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>-4.656613e-11</td>\n",
       "      <td>7.698933e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.658093e+06</td>\n",
       "      <td>169.749977</td>\n",
       "      <td>-1.851063e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>117.786254</td>\n",
       "      <td>0.015714</td>\n",
       "      <td>...</td>\n",
       "      <td>1.782538e+06</td>\n",
       "      <td>258083.413997</td>\n",
       "      <td>1.933665e-05</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>76.792589</td>\n",
       "      <td>0.489672</td>\n",
       "      <td>-4.050418e-07</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>6.364038e-11</td>\n",
       "      <td>-2.483572e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180116e+42</td>\n",
       "      <td>3.096140e+26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.078746e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.142814</td>\n",
       "      <td>0.010842</td>\n",
       "      <td>...</td>\n",
       "      <td>1.107130e+07</td>\n",
       "      <td>4943.190623</td>\n",
       "      <td>-2.452558e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>69.341553</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>1.089206e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.094850e+26</td>\n",
       "      <td>5.820766e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180116e+42</td>\n",
       "      <td>3.096140e+26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.078746e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.141607</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>...</td>\n",
       "      <td>1.107156e+07</td>\n",
       "      <td>4883.483012</td>\n",
       "      <td>9.712145e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>69.345515</td>\n",
       "      <td>0.059899</td>\n",
       "      <td>4.368168e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3.094850e+26</td>\n",
       "      <td>-1.940255e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180116e+42</td>\n",
       "      <td>3.096140e+26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.078746e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.140617</td>\n",
       "      <td>0.011122</td>\n",
       "      <td>...</td>\n",
       "      <td>1.107207e+07</td>\n",
       "      <td>4658.242472</td>\n",
       "      <td>5.985480e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>69.346796</td>\n",
       "      <td>0.059158</td>\n",
       "      <td>1.142284e-06</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>3.094850e+26</td>\n",
       "      <td>1.940255e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180116e+42</td>\n",
       "      <td>3.096140e+26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.078746e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.140404</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>...</td>\n",
       "      <td>1.107242e+07</td>\n",
       "      <td>4581.638458</td>\n",
       "      <td>4.441322e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>69.355407</td>\n",
       "      <td>0.059855</td>\n",
       "      <td>2.365953e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>3.094850e+26</td>\n",
       "      <td>-1.940255e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180116e+42</td>\n",
       "      <td>3.096140e+26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.078746e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.141566</td>\n",
       "      <td>0.011207</td>\n",
       "      <td>...</td>\n",
       "      <td>1.107294e+07</td>\n",
       "      <td>4085.714567</td>\n",
       "      <td>1.059609e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>69.365419</td>\n",
       "      <td>0.058269</td>\n",
       "      <td>2.433330e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>3.094850e+26</td>\n",
       "      <td>1.940255e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13326 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           P-PDG     P-PDG_std  P-PDG_pctchg_mean  P-PDG_pctchg_std   \n",
       "0   0.000000e+00  0.000000e+00                0.0               0.0  \\\n",
       "0   0.000000e+00  0.000000e+00                0.0               0.0   \n",
       "0   0.000000e+00  0.000000e+00                0.0               0.0   \n",
       "0   0.000000e+00  0.000000e+00                0.0               0.0   \n",
       "0   0.000000e+00  0.000000e+00                0.0               0.0   \n",
       "..           ...           ...                ...               ...   \n",
       "0  -1.180116e+42  3.096140e+26                0.0               0.0   \n",
       "0  -1.180116e+42  3.096140e+26                0.0               0.0   \n",
       "0  -1.180116e+42  3.096140e+26                0.0               0.0   \n",
       "0  -1.180116e+42  3.096140e+26                0.0               0.0   \n",
       "0  -1.180116e+42  3.096140e+26                0.0               0.0   \n",
       "\n",
       "           P-TPT     P-TPT_std  P-TPT_pctchg_mean  P-TPT_pctchg_std   \n",
       "0   8.565903e+06  61056.170215       2.078730e-05          0.000011  \\\n",
       "0   8.419693e+06   4233.729173      -1.972196e-06          0.000022   \n",
       "0   8.748236e+06  15403.806555       2.078632e-06          0.000014   \n",
       "0   8.828215e+06  10590.417050      -1.189768e-06          0.000018   \n",
       "0   8.658093e+06    169.749977      -1.851063e-07          0.000001   \n",
       "..           ...           ...                ...               ...   \n",
       "0   2.078746e+07      0.000000       0.000000e+00          0.000000   \n",
       "0   2.078746e+07      0.000000       0.000000e+00          0.000000   \n",
       "0   2.078746e+07      0.000000       0.000000e+00          0.000000   \n",
       "0   2.078746e+07      0.000000       0.000000e+00          0.000000   \n",
       "0   2.078746e+07      0.000000       0.000000e+00          0.000000   \n",
       "\n",
       "         T-TPT  T-TPT_std  ...     P-MON-CKP  P-MON-CKP_std   \n",
       "0   118.041962   0.222718  ...  1.749214e+06  236860.281168  \\\n",
       "0   117.029977   0.006837  ...  1.667997e+06  230222.740329   \n",
       "0   117.459051   0.047106  ...  1.759294e+06  196850.107693   \n",
       "0   117.624216   0.031579  ...  1.759798e+06  200198.675728   \n",
       "0   117.786254   0.015714  ...  1.782538e+06  258083.413997   \n",
       "..         ...        ...  ...           ...            ...   \n",
       "0   118.142814   0.010842  ...  1.107130e+07    4943.190623   \n",
       "0   118.141607   0.010670  ...  1.107156e+07    4883.483012   \n",
       "0   118.140617   0.011122  ...  1.107207e+07    4658.242472   \n",
       "0   118.140404   0.011407  ...  1.107242e+07    4581.638458   \n",
       "0   118.141566   0.011207  ...  1.107294e+07    4085.714567   \n",
       "\n",
       "    P-MON-CKP_pctchg_mean  P-MON-CKP_pctchg_std  T-JUS-CKP  T-JUS-CKP_std   \n",
       "0           -3.093040e-05              0.008582  75.105912       0.472400  \\\n",
       "0            5.592193e-05              0.010094  74.859587       0.356903   \n",
       "0            6.716495e-05              0.008226  75.750729       0.587198   \n",
       "0           -2.761304e-05              0.008616  77.141361       0.478572   \n",
       "0            1.933665e-05              0.011107  76.792589       0.489672   \n",
       "..                    ...                   ...        ...            ...   \n",
       "0           -2.452558e-07              0.000013  69.341553       0.059850   \n",
       "0            9.712145e-07              0.000013  69.345515       0.059899   \n",
       "0            5.985480e-07              0.000013  69.346796       0.059158   \n",
       "0            4.441322e-07              0.000013  69.355407       0.059855   \n",
       "0            1.059609e-06              0.000012  69.365419       0.058269   \n",
       "\n",
       "    T-JUS-CKP_pctchg_mean  T-JUS-CKP_pctchg_std         pca_1         pca_2  \n",
       "0           -2.244456e-06              0.000262 -3.414849e-11  8.723388e-10  \n",
       "0           -6.398129e-06              0.000286 -3.104409e-12  5.217347e-10  \n",
       "0            5.073188e-07              0.000371  1.008933e-10 -5.122274e-11  \n",
       "0           -3.179340e-06              0.000351 -4.656613e-11  7.698933e-10  \n",
       "0           -4.050418e-07              0.000327  6.364038e-11 -2.483572e-10  \n",
       "..                    ...                   ...           ...           ...  \n",
       "0            1.089206e-06              0.000003  3.094850e+26  5.820766e-13  \n",
       "0            4.368168e-07              0.000006  3.094850e+26 -1.940255e-13  \n",
       "0            1.142284e-06              0.000011  3.094850e+26  1.940255e-13  \n",
       "0            2.365953e-06              0.000013  3.094850e+26 -1.940255e-13  \n",
       "0            2.433330e-06              0.000013  3.094850e+26  1.940255e-13  \n",
       "\n",
       "[13326 rows x 22 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['P-PDG', 'P-PDG_std','P-PDG_pctchg_mean', 'P-PDG_pctchg_std',\n",
    "            'P-TPT', 'P-TPT_std', 'P-TPT_pctchg_mean', 'P-TPT_pctchg_std',\n",
    "            'T-TPT', 'T-TPT_std', 'T-TPT_pctchg_mean', 'T-TPT_pctchg_std',\n",
    "            'P-MON-CKP', 'P-MON-CKP_std', 'P-MON-CKP_pctchg_mean', 'P-MON-CKP_pctchg_std',\n",
    "            'T-JUS-CKP', 'T-JUS-CKP_std', 'T-JUS-CKP_pctchg_mean', 'T-JUS-CKP_pctchg_std',\n",
    "            'pca_1', 'pca_2']\n",
    "X_train_pre, y_train = merged_df_train.loc[:,features], merged_df_train['class']\n",
    "X_train_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "# Pandas automatically applies colomn-wise function in the code above.\n",
    "# merged_new_df = (merged_df - merged_df.mean())/merged_df.std()\n",
    "features = ['P-PDG', 'P-PDG_std','P-PDG_pctchg_mean', 'P-PDG_pctchg_std',\n",
    "            'P-TPT', 'P-TPT_std', 'P-TPT_pctchg_mean', 'P-TPT_pctchg_std',\n",
    "            'T-TPT', 'T-TPT_std', 'T-TPT_pctchg_mean', 'T-TPT_pctchg_std',\n",
    "            'P-MON-CKP', 'P-MON-CKP_std', 'P-MON-CKP_pctchg_mean', 'P-MON-CKP_pctchg_std',\n",
    "            'T-JUS-CKP', 'T-JUS-CKP_std', 'T-JUS-CKP_pctchg_mean', 'T-JUS-CKP_pctchg_std',\n",
    "            'pca_1', 'pca_2']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_pre, y_train = merged_df_train.loc[:,features], merged_df_train['class']\n",
    "X_val_pre, y_val = merged_df_val.loc[:,features], merged_df_val['class']\n",
    "X_test_pre, y_test = merged_df_test.loc[:,features], merged_df_test['class']\n",
    "\n",
    "# 각 칼럼의 std는 정규화에서 제외\n",
    "columns = ['P-PDG', 'P-PDG_pctchg_mean','P-TPT','P-TPT_pctchg_mean', 'T-TPT','T-TPT_pctchg_mean', 'P-MON-CKP','P-MON-CKP_pctchg_mean', 'T-JUS-CKP','T-JUS-CKP_pctchg_mean',\n",
    "           'P-PDG_std', 'P-TPT_std', 'T-TPT_std', 'P-MON-CKP_std', 'T-JUS-CKP_std',\n",
    "           'pca_1', 'pca_2']\n",
    "\n",
    "X_train_pre.loc[:,columns] = (X_train_pre.loc[:,columns] - X_train_pre.loc[:,columns].mean())/X_train_pre.loc[:,columns].std()\n",
    "X_val_pre.loc[:,columns] = (X_val_pre.loc[:,columns] - X_val_pre.loc[:,columns].mean())/X_val_pre.loc[:,columns].std()\n",
    "X_test_pre.loc[:,columns] = (X_test_pre.loc[:,columns] - X_test_pre.loc[:,columns].mean())/X_test_pre.loc[:,columns].std()\n",
    "\n",
    "X_train = X_train_pre\n",
    "X_val = X_val_pre\n",
    "X_test = X_test_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train \u001b[39m=\u001b[39m X_train[features]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m y_train \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m      5\u001b[0m X_val \u001b[39m=\u001b[39m X_val[features]\u001b[39m.\u001b[39mvalues\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# training\n",
    "X_train = X_train[features].values\n",
    "y_train = y_train.values.astype(int)\n",
    "\n",
    "X_val = X_val[features].values\n",
    "y_val = y_val.values.astype(int)\n",
    "\n",
    "X_test = X_test[features].values\n",
    "y_test = y_test.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "optimizer_fn = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=0.0001)\n",
    "batch_size = 64\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW, 0.7,  0.15, 0.15, long = 1200 short = 1200, 0.95, lr = {'lr': 0.0001}, batch_size = 64, no_weigth_loss, patience = 10, val_acc = ?, test_acc = ?, no Norm, no Simul\n",
      "0 is in event, 10070\n",
      "2 is in event, 58\n",
      "3 is in event, 565\n",
      "4 is in event, 2301\n",
      "5 is in event, 70\n",
      "6 is in event, 198\n",
      "7 is in event, 64\n",
      "tensor([0.2443, 0.9956, 0.9576, 0.8273, 0.9947, 0.9851, 0.9952])\n",
      "epoch 0  | loss: 2.69135 | val_0_accuracy: 0.27839 |  0:00:06s\n",
      "epoch 1  | loss: 2.5573  | val_0_accuracy: 0.37937 |  0:00:11s\n",
      "epoch 2  | loss: 2.36449 | val_0_accuracy: 0.47437 |  0:00:17s\n",
      "epoch 3  | loss: 2.24265 | val_0_accuracy: 0.48118 |  0:00:23s\n",
      "epoch 4  | loss: 2.11222 | val_0_accuracy: 0.39928 |  0:00:29s\n",
      "epoch 5  | loss: 2.04116 | val_0_accuracy: 0.3405  |  0:00:35s\n",
      "epoch 6  | loss: 1.94527 | val_0_accuracy: 0.33543 |  0:00:41s\n",
      "epoch 7  | loss: 1.84265 | val_0_accuracy: 0.34008 |  0:00:47s\n",
      "epoch 8  | loss: 1.78735 | val_0_accuracy: 0.35988 |  0:00:52s\n",
      "epoch 9  | loss: 1.68423 | val_0_accuracy: 0.35684 |  0:00:58s\n",
      "epoch 10 | loss: 1.63514 | val_0_accuracy: 0.3652  |  0:01:04s\n",
      "epoch 11 | loss: 1.59452 | val_0_accuracy: 0.35303 |  0:01:10s\n",
      "epoch 12 | loss: 1.51374 | val_0_accuracy: 0.37993 |  0:01:16s\n",
      "epoch 13 | loss: 1.48711 | val_0_accuracy: 0.38808 |  0:01:22s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_accuracy = 0.48118\n"
     ]
    }
   ],
   "source": [
    "print(f'{optimizer_fn.__name__}, {train_ratio}, {val_ratio: .2f}, {test_ratio}, long = {window_size_long} short = {window_size_short}, {overlap_ratio}, lr = {optimizer_params}, batch_size = {batch_size}, no_weigth_loss, patience = {patience}, val_acc = ?, test_acc = ?, no Norm, no Simul')\n",
    "\n",
    "weight_for_class = []\n",
    "total_obs = len(y_train)\n",
    "# check\n",
    "for key, value in sorted(Counter(y_train).items()):\n",
    "    print(f'{key} is in event, {value}')\n",
    "    weight_for_class.append(1 - (value/total_obs))\n",
    "\n",
    "max_epochs = 200\n",
    "weight_for_class = torch.Tensor(weight_for_class).type(torch.float32)\n",
    "print(weight_for_class)\n",
    "\n",
    "clf = TabNetClassifier(\n",
    "    optimizer_fn=optimizer_fn,\n",
    "    optimizer_params=optimizer_params,#1e-2\n",
    "    # cat_emb_dim = [window_size, window_size, window_size, window_size, window_size]\n",
    "    )  #TabNetRegressor()\n",
    "\n",
    "result = clf.fit(\n",
    "    X_train = X_train,\n",
    "    y_train = y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric = [\"accuracy\", ],\n",
    "    loss_fn = nn.CrossEntropyLoss(weight_for_class),\n",
    "    # loss_fn = nn.CrossEntropyLoss(),\n",
    "    batch_size = batch_size,\n",
    "    max_epochs = max_epochs,\n",
    "    drop_last = True,\n",
    "    patience = patience\n",
    ")\n",
    "# AdamW, 0.7,  0.15, 0.15, long = 1200 short = 1200, 0.95, lr = {'lr': 0.001}, batch_size = 64, no_weigth_loss, patience = 10, val_acc = 0.96924, test_acc = 0.954, no Norm, no Simul\n",
    "# AdamW, 0.7,  0.15, 0.15, long = 1200 short = 1200, 0.95, lr = {'lr': 0.001}, batch_size = 128, no_weigth_loss, patience = 15, val_acc = .883, test_acc = .957, no Norm, no Simul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9618039904154113\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     31604\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.97      0.96      0.96      1569\n",
      "           4       0.86      1.00      0.92      8919\n",
      "           5       0.00      0.00      0.00        30\n",
      "           6       0.00      0.00      0.00        10\n",
      "           7       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96     42151\n",
      "   macro avg       0.40      0.42      0.41     42151\n",
      "weighted avg       0.97      0.96      0.96     42151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "print(accuracy_score(preds, y_test))\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving_path_name = './tabnet_test_acc_9904'\n",
    "# clf.save_model(saving_path_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
