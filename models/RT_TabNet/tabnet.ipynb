{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/louan/Documents/projects/Aiffel/Hackathon/geodata-con/geo-con/models/RT_TabNet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler # 표준화 패키지 라이브러리\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from warnings import filterwarnings\n",
    "import gc\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filterwarnings('ignore')\n",
    "device = torch.device('cpu')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_integration as di"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../3W/dataset/0/*.csv\n",
      "../../../3W/dataset/1/*.csv\n",
      "../../../3W/dataset/2/*.csv\n",
      "../../../3W/dataset/3/*.csv\n",
      "../../../3W/dataset/4/*.csv\n",
      "../../../3W/dataset/5/*.csv\n",
      "../../../3W/dataset/6/*.csv\n",
      "../../../3W/dataset/7/*.csv\n",
      "../../../3W/dataset/8/*.csv\n"
     ]
    }
   ],
   "source": [
    "# 전체 file list 만들기\n",
    "path = '../../../3W/dataset/' # the path of 3W/dataset/\n",
    "for event in range(9):\n",
    "    file_lst = []\n",
    "    full_path = path + f'{event}/*.csv'\n",
    "    print(full_path)\n",
    "    file_lst.extend(sorted(glob(full_path)))\n",
    "    globals()['df_event_{}'.format(event)] = di.dt_integration(file_lst[:20])\n",
    "    globals()['df_event_{}'.format(event)].drop(columns = ['timestamp', 'event_type', 'P-JUS-CKGL', 'T-JUS-CKGL', 'QGL', 'instance_type'], inplace = True)\n",
    "    globals()['df_event_{}'.format(event)].reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P-PDG</th>\n",
       "      <th>P-TPT</th>\n",
       "      <th>T-TPT</th>\n",
       "      <th>P-MON-CKP</th>\n",
       "      <th>T-JUS-CKP</th>\n",
       "      <th>class</th>\n",
       "      <th>id_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19426080.0</td>\n",
       "      <td>12508490.0</td>\n",
       "      <td>121.7742</td>\n",
       "      <td>4034498.0</td>\n",
       "      <td>86.99649</td>\n",
       "      <td>3</td>\n",
       "      <td>SIMULATED_00001_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19427790.0</td>\n",
       "      <td>12510300.0</td>\n",
       "      <td>121.7738</td>\n",
       "      <td>4034516.0</td>\n",
       "      <td>86.99896</td>\n",
       "      <td>3</td>\n",
       "      <td>SIMULATED_00001_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19429510.0</td>\n",
       "      <td>12512120.0</td>\n",
       "      <td>121.7734</td>\n",
       "      <td>4034534.0</td>\n",
       "      <td>87.00143</td>\n",
       "      <td>3</td>\n",
       "      <td>SIMULATED_00001_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19431230.0</td>\n",
       "      <td>12513930.0</td>\n",
       "      <td>121.7733</td>\n",
       "      <td>4034553.0</td>\n",
       "      <td>87.00390</td>\n",
       "      <td>3</td>\n",
       "      <td>SIMULATED_00001_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19432940.0</td>\n",
       "      <td>12515750.0</td>\n",
       "      <td>121.7730</td>\n",
       "      <td>4034570.0</td>\n",
       "      <td>87.00637</td>\n",
       "      <td>3</td>\n",
       "      <td>SIMULATED_00001_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125975</th>\n",
       "      <td>17411620.0</td>\n",
       "      <td>10125210.0</td>\n",
       "      <td>105.6147</td>\n",
       "      <td>4083094.0</td>\n",
       "      <td>45.40679</td>\n",
       "      <td>3</td>\n",
       "      <td>SIMULATED_00020_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125976</th>\n",
       "      <td>17411820.0</td>\n",
       "      <td>10125280.0</td>\n",
       "      <td>105.6101</td>\n",
       "      <td>4083095.0</td>\n",
       "      <td>45.41088</td>\n",
       "      <td>3</td>\n",
       "      <td>SIMULATED_00020_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125977</th>\n",
       "      <td>17412020.0</td>\n",
       "      <td>10125350.0</td>\n",
       "      <td>105.6054</td>\n",
       "      <td>4083094.0</td>\n",
       "      <td>45.41497</td>\n",
       "      <td>3</td>\n",
       "      <td>SIMULATED_00020_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125978</th>\n",
       "      <td>17412230.0</td>\n",
       "      <td>10125420.0</td>\n",
       "      <td>105.6007</td>\n",
       "      <td>4083093.0</td>\n",
       "      <td>45.41905</td>\n",
       "      <td>3</td>\n",
       "      <td>SIMULATED_00020_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125979</th>\n",
       "      <td>17412440.0</td>\n",
       "      <td>10125480.0</td>\n",
       "      <td>105.5960</td>\n",
       "      <td>4083093.0</td>\n",
       "      <td>45.42312</td>\n",
       "      <td>3</td>\n",
       "      <td>SIMULATED_00020_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1125980 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              P-PDG       P-TPT     T-TPT  P-MON-CKP  T-JUS-CKP  class  \\\n",
       "0        19426080.0  12508490.0  121.7742  4034498.0   86.99649      3   \n",
       "1        19427790.0  12510300.0  121.7738  4034516.0   86.99896      3   \n",
       "2        19429510.0  12512120.0  121.7734  4034534.0   87.00143      3   \n",
       "3        19431230.0  12513930.0  121.7733  4034553.0   87.00390      3   \n",
       "4        19432940.0  12515750.0  121.7730  4034570.0   87.00637      3   \n",
       "...             ...         ...       ...        ...        ...    ...   \n",
       "1125975  17411620.0  10125210.0  105.6147  4083094.0   45.40679      3   \n",
       "1125976  17411820.0  10125280.0  105.6101  4083095.0   45.41088      3   \n",
       "1125977  17412020.0  10125350.0  105.6054  4083094.0   45.41497      3   \n",
       "1125978  17412230.0  10125420.0  105.6007  4083093.0   45.41905      3   \n",
       "1125979  17412440.0  10125480.0  105.5960  4083093.0   45.42312      3   \n",
       "\n",
       "                  id_label  \n",
       "0        SIMULATED_00001_3  \n",
       "1        SIMULATED_00001_3  \n",
       "2        SIMULATED_00001_3  \n",
       "3        SIMULATED_00001_3  \n",
       "4        SIMULATED_00001_3  \n",
       "...                    ...  \n",
       "1125975  SIMULATED_00020_3  \n",
       "1125976  SIMULATED_00020_3  \n",
       "1125977  SIMULATED_00020_3  \n",
       "1125978  SIMULATED_00020_3  \n",
       "1125979  SIMULATED_00020_3  \n",
       "\n",
       "[1125980 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "df_event_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interplolate_without_class(df, method='linear'):\n",
    "    '''\n",
    "    input (df) : 라벨 단위 observation_df\n",
    "    output (df) : 라벨 단위 observation_df\n",
    "    기본 보간 방법은 선형 보간\n",
    "    '''\n",
    "    columns = df.columns\n",
    "    columns = columns.drop('class')\n",
    "    class_observations = df['class']\n",
    "\n",
    "    df = df[columns].interpolate(method = method)\n",
    "    df['class'] = class_observations\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10\n",
    "events = 9 # 0부터 8번까지의 이벤트를 따로따로 저장함\n",
    "\n",
    "before_interpolation_description = []\n",
    "after_interpolation_description = []\n",
    "observation_nums_lst = []\n",
    "\n",
    "for event in range(event):\n",
    "    #1. 인스턴스 별 칼럼의 결측 비율 조사\n",
    "    df = globals()['df_event_{}'.format(event)]\n",
    "    missing_proportion_df = di.missing_data_proportion(df)\n",
    "\n",
    "    #2. Threshold 이상의 observation만 남기고 나머지 observation 제거\n",
    "    index = missing_proportion_df[(missing_proportion_df > threshold).sum(axis=1) == 0].index\n",
    "    print(index)\n",
    "    df = df[df['id_label'].isin(index)]\n",
    "    print(f\"{event} 라벨의 최종 관측 개수: {len(df)}\")\n",
    "\n",
    "    #3. 라벨 제외한 df 보간\n",
    "    globals()['df_event_{}'.format(event)] = interplolate_without_class(df, method = 'linear')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../data/pre_processed_split_data_0.pkl', '../../../data/pre_processed_split_data_1.pkl', '../../../data/pre_processed_split_data_2.pkl', '../../../data/pre_processed_split_data_3.pkl', '../../../data/pre_processed_split_data_4.pkl', '../../../data/pre_processed_split_data_5.pkl', '../../../data/pre_processed_split_0tpt_data_6.pkl', '../../../data/pre_processed_split_data_7.pkl', '../../../data/pre_processed_split_data_8.pkl']\n"
     ]
    }
   ],
   "source": [
    "# path = '../../../data/*.pkl'\n",
    "\n",
    "# file_lst = sorted(glob(path))[1:]\n",
    "# file_lst[6] = '../../../data/pre_processed_split_0tpt_data_6.pkl'\n",
    "# print(file_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ../../../data/pre_processed_split_data_0.pkl\n",
      "1 ../../../data/pre_processed_split_data_1.pkl\n",
      "2 ../../../data/pre_processed_split_data_2.pkl\n",
      "3 ../../../data/pre_processed_split_data_3.pkl\n",
      "4 ../../../data/pre_processed_split_data_4.pkl\n",
      "5 ../../../data/pre_processed_split_data_5.pkl\n",
      "6 ../../../data/pre_processed_split_0tpt_data_6.pkl\n",
      "7 ../../../data/pre_processed_split_data_7.pkl\n",
      "8 ../../../data/pre_processed_split_data_8.pkl\n"
     ]
    }
   ],
   "source": [
    "# # ModuleNotFoundError: No module named 'pandas.core.indexes.numeric' using Metaflow\n",
    "# # This issue is caused by the new Pandas 2.0.0 release => pip install \"pandas<2.0.0\"\n",
    "# # https://stackoverflow.com/questions/75953279/modulenotfounderror-no-module-named-pandas-core-indexes-numeric-using-metaflo\n",
    "# for i,path in enumerate(file_lst):\n",
    "#     print(i, path)\n",
    "#     with open(path, 'rb') as f:\n",
    "#         globals()['df_event_{}'.format(i)] = pickle.load(f)\n",
    "#         globals()['df_event_{}'.format(i)].reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier 제거\n",
    "- 참고: https://stackoverflow.com/questions/35827863/remove-outliers-in-pandas-dataframe-using-percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add delta and remove outlier\n",
    "def add_delta(data_frame):\n",
    "    for column in ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']:\n",
    "        column_pctchg = column+'_pctchg'\n",
    "        data_frame[column_pctchg] = data_frame.groupby(['id_label'])[column].pct_change()\n",
    "        data_frame[column_pctchg][data_frame[column_pctchg].isna()] = 0\n",
    "    return data_frame\n",
    "\n",
    "def remove_outlier(df):\n",
    "    target_cols = ['P-PDG', 'P-TPT', 'T-TPT','P-MON-CKP', 'T-JUS-CKP',]\n",
    "    df = df[df['id_label'].str.startswith('WELL')]\n",
    "    df = add_delta(df)\n",
    "    Q1 = df[target_cols].quantile(0.25)\n",
    "    Q3 = df[target_cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df[target_cols] < (Q1 - 1.5 * IQR)) | (df[target_cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print(f'event_num: {i}')\n",
    "    df_event_0_out = remove_outlier(df_event_0)\n",
    "    df_event_1_out = remove_outlier(df_event_1)\n",
    "    df_event_2_out = remove_outlier(df_event_2)\n",
    "    df_event_3_out = remove_outlier(df_event_3)\n",
    "    df_event_4_out = remove_outlier(df_event_4)\n",
    "    df_event_5_out = remove_outlier(df_event_5)\n",
    "    df_event_6_out = remove_outlier(df_event_6)\n",
    "    df_event_7_out = remove_outlier(df_event_7)\n",
    "    df_event_8_out = remove_outlier(df_event_8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the list of each data type(WELL, SIMULATED, DRAWN)\n",
    "instance_dict = {'WELL': [], 'SIMULATED': [], 'DRAWN': []}\n",
    "for i in range(9):\n",
    "  instance_lst = list(globals()['df_event_{}'.format(i)]['id_label'].unique())\n",
    "  for instance in instance_lst:\n",
    "    if instance.startswith('WELL'):\n",
    "      instance_dict['WELL'].extend([instance])\n",
    "    elif instance.startswith('SIMULATED'):\n",
    "      instance_dict['SIMULATED'].extend([instance])\n",
    "    elif instance.startswith('DRAWN'):\n",
    "      instance_dict['DRAWN'].extend([instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 20, '4': 20, '7': 5, '2': 4})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of events in 'WELL' data\n",
    "class_lst = []\n",
    "for i in instance_dict['WELL']:\n",
    "  class_lst.append(i.split('_')[-1])\n",
    "Counter(class_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test instances\n",
    "def split_train_test_instance(instance_dict, dTypes = ['WELL', 'SIMULATED', 'DRAWN'],train_ratio = 0.6, test_ratio = 0.2):\n",
    "    total_len = 0\n",
    "    simul_drawn_lst = []\n",
    "    for dType in dTypes:\n",
    "        total_len += len(instance_dict[dType])\n",
    "        simul_drawn_lst += instance_dict[dType]\n",
    "    well_len = len(instance_dict['WELL'])\n",
    "\n",
    "    num_of_test = int(total_len*test_ratio)\n",
    "    num_of_val = int(total_len*(1-train_ratio-test_ratio))\n",
    "    num_of_train = total_len - num_of_test - num_of_val\n",
    "\n",
    "    # extract test_lst first(Only WELL data)\n",
    "    test_lst = random.sample(instance_dict['WELL'], num_of_test)\n",
    "    \n",
    "    # then make whole lst --> shuffle lst --> split train validation set\n",
    "    rest_well_lst = [i for i in instance_dict['WELL'] if i not in test_lst]\n",
    "    total_lst = rest_well_lst + simul_drawn_lst\n",
    "    random.shuffle(total_lst)\n",
    "\n",
    "    train_lst = total_lst[:num_of_train]\n",
    "    val_lst = total_lst[num_of_train:]\n",
    "\n",
    "    return train_lst, val_lst, test_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "75\n",
      "26\n",
      "total number of lst: 175\n",
      "total number of lst after removing duplicated: 175\n",
      "train:\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
      "val:\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
      "test:\n",
      "['0', '2', '4', '7']\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "val_ratio = 1 - train_ratio - test_ratio\n",
    "dTypes = ['WELL', 'SIMULATED', 'DRAWN']\n",
    "train_lst, val_lst, test_lst = split_train_test_instance(instance_dict, dTypes = dTypes, train_ratio = train_ratio, test_ratio = test_ratio)\n",
    "\n",
    "print(len(train_lst), len(val_lst), len(test_lst), sep = '\\n')\n",
    "print('total number of lst:', len(instance_dict['WELL'])+len(instance_dict['DRAWN'])+len(instance_dict['SIMULATED']))\n",
    "print('total number of lst after removing duplicated:', len(set(train_lst + val_lst + test_lst)))\n",
    "print('train:', sorted(set([i.split('_')[-1] for i in train_lst])), 'val:', sorted(set([i.split('_')[-1] for i in val_lst])), 'test:', sorted(set([i.split('_')[-1] for i in test_lst])), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: https://rfriend.tistory.com/590\n",
    "def make_empty_df(pca_n_components):\n",
    "    pca_lst = ['pca_1', 'pca_2','pca_3', 'pca_4', 'pca_5']\n",
    "    columns = ['P-PDG', 'P-PDG_std','P-PDG_pctchg_mean', 'P-PDG_pctchg_std',\n",
    "               'P-TPT', 'P-TPT_std', 'P-TPT_pctchg_mean', 'P-TPT_pctchg_std',\n",
    "               'T-TPT', 'T-TPT_std', 'T-TPT_pctchg_mean', 'T-TPT_pctchg_std',\n",
    "               'P-MON-CKP', 'P-MON-CKP_std', 'P-MON-CKP_pctchg_mean', 'P-MON-CKP_pctchg_std',\n",
    "               'T-JUS-CKP', 'T-JUS-CKP_std', 'T-JUS-CKP_pctchg_mean', 'T-JUS-CKP_pctchg_std',\n",
    "               'class',]\n",
    "    columns.extend(pca_lst[:pca_n_components])\n",
    "    data_frame = pd.DataFrame(columns=columns)\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def apply_window(data_frame, window_size = 5000, overlap_ratio = 0.2, pca_n_components = 2):\n",
    "    if pca_n_components:\n",
    "        pca = PCA(n_components=pca_n_components)\n",
    "    \n",
    "    new_df = make_empty_df(pca_n_components)\n",
    "    for n in tqdm(range(int(len(data_frame)/(window_size*(1-overlap_ratio))))):\n",
    "        try:\n",
    "            if int(window_size*(n+1) - window_size*overlap_ratio*n) <= len(data_frame):\n",
    "                # print(window_size*n*(1-overlap_ratio),window_size*(n+1) - window_size*overlap_ratio*n)\n",
    "                sliced_df = data_frame[int(window_size*n*(1-overlap_ratio)):int(window_size*(n+1) - window_size*overlap_ratio*n)]\n",
    "                pca_data = pca.fit_transform(sliced_df.drop( ['id_label','class'], axis=1))\n",
    "                if any(sliced_df['class'] != 0):\n",
    "                    class_label =  int(sliced_df['class'].value_counts().index[sliced_df['class'].value_counts().index < 10][0]) # 기본 모드\n",
    "                    \n",
    "                    # class_label = 1\n",
    "                else:\n",
    "                    # continue\n",
    "                    class_label = 0\n",
    "                new_df = pd.concat([new_df,pd.DataFrame({'P-PDG':[sliced_df['P-PDG'].mean()],\n",
    "                                                        'P-PDG_std':[sliced_df['P-PDG'].std()],\n",
    "                                                        'P-PDG_pctchg_mean':[sliced_df['P-PDG_pctchg'].mean()],\n",
    "                                                        'P-PDG_pctchg_std':[sliced_df['P-PDG_pctchg'].std()],\n",
    "                                                        'P-TPT':[sliced_df['P-TPT'].mean()],\n",
    "                                                        'P-TPT_std':[sliced_df['P-TPT'].std()],\n",
    "                                                        'P-TPT_pctchg_mean':[sliced_df['P-TPT_pctchg'].mean()],\n",
    "                                                        'P-TPT_pctchg_std':[sliced_df['P-TPT_pctchg'].std()],\n",
    "                                                        'T-TPT':[sliced_df['T-TPT'].mean()],\n",
    "                                                        'T-TPT_std':[sliced_df['T-TPT'].std()],\n",
    "                                                        'T-TPT_pctchg_mean':[sliced_df['T-TPT_pctchg'].mean()],\n",
    "                                                        'T-TPT_pctchg_std':[sliced_df['T-TPT_pctchg'].std()],\n",
    "                                                        'P-MON-CKP':[sliced_df['P-MON-CKP'].mean()],\n",
    "                                                        'P-MON-CKP_std':[sliced_df['P-MON-CKP'].std()],\n",
    "                                                        'P-MON-CKP_pctchg_mean':[sliced_df['P-MON-CKP_pctchg'].mean()],\n",
    "                                                        'P-MON-CKP_pctchg_std':[sliced_df['P-MON-CKP_pctchg'].std()],\n",
    "                                                        'T-JUS-CKP':[sliced_df['T-JUS-CKP'].mean()],\n",
    "                                                        'T-JUS-CKP_std':[sliced_df['T-JUS-CKP'].std()],\n",
    "                                                        'T-JUS-CKP_pctchg_mean':[sliced_df['T-JUS-CKP_pctchg'].mean()],\n",
    "                                                        'T-JUS-CKP_pctchg_std':[sliced_df['T-JUS-CKP_pctchg'].std()],\n",
    "                                                        'class': class_label,\n",
    "                                                        'pca_1':[pca_data[:,0].mean()],\n",
    "                                                        'pca_2':[pca_data[:,1].mean()],\n",
    "                                                        #'pca_3':[pca_data.iloc[:,2].mean()],\n",
    "                                                        })])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    return new_df\n",
    "\n",
    "def build_dataset(data_frame, train_lst, val_lst, test_lst, window_size = 5000, overlap_ratio = 0.2, pca_n_components = 2):\n",
    "  gc.collect()\n",
    "  train_data_frame = data_frame[data_frame['id_label'].isin(train_lst)]\n",
    "  val_data_frame = data_frame[data_frame['id_label'].isin(val_lst)]\n",
    "  test_data_frame = data_frame[data_frame['id_label'].isin(test_lst)]\n",
    "\n",
    "#   train_data_frame = remove_outlier(train_data_frame)\n",
    "\n",
    "  train_df = apply_window(train_data_frame, window_size = window_size, overlap_ratio = overlap_ratio)\n",
    "  val_df = apply_window(val_data_frame, window_size = window_size, overlap_ratio = overlap_ratio)\n",
    "  test_df = apply_window(test_data_frame, window_size = window_size, overlap_ratio = overlap_ratio)\n",
    "  return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [00:24<00:00, 174.64it/s]\n",
      "100%|██████████| 3279/3279 [00:19<00:00, 169.84it/s]\n",
      "100%|██████████| 3342/3342 [00:19<00:00, 172.58it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 387/387 [00:00<00:00, 1852.26it/s]\n",
      "100%|██████████| 328/328 [00:00<00:00, 2013.05it/s]\n",
      "100%|██████████| 261/261 [00:00<00:00, 2054.59it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1675/1675 [00:10<00:00, 160.03it/s]\n",
      "100%|██████████| 1121/1121 [00:07<00:00, 152.91it/s]\n",
      "100%|██████████| 1138/1138 [00:07<00:00, 156.09it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 5730/5730 [00:28<00:00, 200.01it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 4513/4513 [00:20<00:00, 215.88it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying window\n",
    "window_size = 1200\n",
    "window_size_short = window_size\n",
    "window_size_long =  window_size\n",
    "pca_n_components = 2\n",
    "overlap_ratio = 0.95\n",
    "\n",
    "df_event_0_window_train, df_event_0_window_val, df_event_0_window_test = build_dataset(df_event_0_out, train_lst, val_lst, test_lst, window_size = window_size_long, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_1_window_train, df_event_1_window_val, df_event_1_window_test = build_dataset(df_event_1_out, train_lst, val_lst, test_lst, window_size = window_size_long, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_2_window_train, df_event_2_window_val, df_event_2_window_test = build_dataset(df_event_2_out, train_lst, val_lst, test_lst, window_size = window_size_short, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_3_window_train, df_event_3_window_val, df_event_3_window_test = build_dataset(df_event_3_out, train_lst, val_lst, test_lst, window_size = window_size_long, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_4_window_train, df_event_4_window_val, df_event_4_window_test = build_dataset(df_event_4_out, train_lst, val_lst, test_lst, window_size = window_size_short, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_5_window_train, df_event_5_window_val, df_event_5_window_test = build_dataset(df_event_5_out, train_lst, val_lst, test_lst, window_size = window_size_long, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_6_window_train, df_event_6_window_val, df_event_6_window_test = build_dataset(df_event_6_out, train_lst, val_lst, test_lst, window_size = window_size_short, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_7_window_train, df_event_7_window_val, df_event_7_window_test = build_dataset(df_event_7_out, train_lst, val_lst, test_lst, window_size = window_size_long, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "df_event_8_window_train, df_event_8_window_val, df_event_8_window_test = build_dataset(df_event_8_out, train_lst, val_lst, test_lst, window_size = window_size_short, overlap_ratio = overlap_ratio, pca_n_components = pca_n_components)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling for train dataset\n",
    "merged_df_train =  pd.concat([df_event_0_window_train,\n",
    "                              df_event_1_window_train,\n",
    "                              df_event_2_window_train,\n",
    "                              df_event_3_window_train,\n",
    "                              df_event_4_window_train,\n",
    "                              df_event_5_window_train,\n",
    "                              df_event_6_window_train,\n",
    "                              df_event_7_window_train,\n",
    "                              df_event_8_window_train])\n",
    "\n",
    "merged_df_val =  pd.concat([df_event_0_window_val,\n",
    "                            df_event_1_window_val,\n",
    "                            df_event_2_window_val,\n",
    "                            df_event_3_window_val,\n",
    "                            df_event_4_window_val,\n",
    "                            df_event_5_window_val,\n",
    "                            df_event_6_window_val,\n",
    "                            df_event_7_window_val,\n",
    "                            df_event_8_window_val])\n",
    "\n",
    "merged_df_test =  pd.concat([df_event_0_window_test,\n",
    "                             df_event_1_window_test,\n",
    "                             df_event_2_window_test,\n",
    "                             df_event_3_window_test,\n",
    "                             df_event_4_window_test,\n",
    "                             df_event_5_window_test,\n",
    "                             df_event_6_window_test,\n",
    "                             df_event_7_window_test,\n",
    "                             df_event_8_window_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6094\n",
      "4    1656\n",
      "7      84\n",
      "Name: class, dtype: int64\n",
      "0    3260\n",
      "4    1102\n",
      "Name: class, dtype: int64\n",
      "0    4579\n",
      "4    1119\n",
      "7      59\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df_train['class'].value_counts())\n",
    "print(merged_df_val['class'].value_counts())\n",
    "print(merged_df_test['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-PDG_pctchg_mean = -inf, 혹은 inf인 row 제외: train(14개), val(10개), test(13)\n",
    "merged_df_train = merged_df_train[~merged_df_train['P-PDG_pctchg_std'].isna()]\n",
    "merged_df_val = merged_df_val[~merged_df_val['P-PDG_pctchg_std'].isna()]\n",
    "merged_df_test = merged_df_test[~merged_df_test['P-PDG_pctchg_std'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P-PDG</th>\n",
       "      <th>P-PDG_std</th>\n",
       "      <th>P-PDG_pctchg_mean</th>\n",
       "      <th>P-PDG_pctchg_std</th>\n",
       "      <th>P-TPT</th>\n",
       "      <th>P-TPT_std</th>\n",
       "      <th>P-TPT_pctchg_mean</th>\n",
       "      <th>P-TPT_pctchg_std</th>\n",
       "      <th>T-TPT</th>\n",
       "      <th>T-TPT_std</th>\n",
       "      <th>...</th>\n",
       "      <th>P-MON-CKP</th>\n",
       "      <th>P-MON-CKP_std</th>\n",
       "      <th>P-MON-CKP_pctchg_mean</th>\n",
       "      <th>P-MON-CKP_pctchg_std</th>\n",
       "      <th>T-JUS-CKP</th>\n",
       "      <th>T-JUS-CKP_std</th>\n",
       "      <th>T-JUS-CKP_pctchg_mean</th>\n",
       "      <th>T-JUS-CKP_pctchg_std</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.994886e+06</td>\n",
       "      <td>10897.730268</td>\n",
       "      <td>-8.660628e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>119.016257</td>\n",
       "      <td>1.142084e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.585979e+06</td>\n",
       "      <td>180428.771377</td>\n",
       "      <td>1.753965e-04</td>\n",
       "      <td>0.014406</td>\n",
       "      <td>84.186317</td>\n",
       "      <td>0.465277</td>\n",
       "      <td>5.615744e-06</td>\n",
       "      <td>1.817294e-04</td>\n",
       "      <td>-1.086543e-11</td>\n",
       "      <td>-5.044664e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.994211e+06</td>\n",
       "      <td>10451.324378</td>\n",
       "      <td>-1.382307e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>119.018544</td>\n",
       "      <td>6.082225e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.577517e+06</td>\n",
       "      <td>176934.340221</td>\n",
       "      <td>2.961627e-04</td>\n",
       "      <td>0.014392</td>\n",
       "      <td>84.181996</td>\n",
       "      <td>0.456952</td>\n",
       "      <td>-1.779371e-06</td>\n",
       "      <td>1.674434e-04</td>\n",
       "      <td>-7.372970e-11</td>\n",
       "      <td>3.228585e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.993274e+06</td>\n",
       "      <td>9473.848196</td>\n",
       "      <td>-1.543777e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>119.019854</td>\n",
       "      <td>1.916549e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.595649e+06</td>\n",
       "      <td>183986.562193</td>\n",
       "      <td>2.098396e-04</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>84.203006</td>\n",
       "      <td>0.478596</td>\n",
       "      <td>1.343774e-05</td>\n",
       "      <td>1.748941e-04</td>\n",
       "      <td>7.838632e-11</td>\n",
       "      <td>-4.889444e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.992425e+06</td>\n",
       "      <td>8488.752857</td>\n",
       "      <td>-1.291574e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>119.020200</td>\n",
       "      <td>1.421678e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.577387e+06</td>\n",
       "      <td>191753.560209</td>\n",
       "      <td>2.359720e-04</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>84.194683</td>\n",
       "      <td>0.508876</td>\n",
       "      <td>-1.607210e-05</td>\n",
       "      <td>2.144586e-04</td>\n",
       "      <td>-3.725290e-11</td>\n",
       "      <td>1.984881e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.991727e+06</td>\n",
       "      <td>7652.170966</td>\n",
       "      <td>-1.038462e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>119.020200</td>\n",
       "      <td>1.421678e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.583204e+06</td>\n",
       "      <td>188257.219081</td>\n",
       "      <td>1.326336e-04</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>84.144764</td>\n",
       "      <td>0.538046</td>\n",
       "      <td>2.366519e-06</td>\n",
       "      <td>2.196625e-04</td>\n",
       "      <td>-3.880511e-11</td>\n",
       "      <td>-3.213063e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180116e+42</td>\n",
       "      <td>3.096140e+26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.114871e+07</td>\n",
       "      <td>4909.096785</td>\n",
       "      <td>5.161176e-11</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>117.638607</td>\n",
       "      <td>1.284836e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.209924e+07</td>\n",
       "      <td>21268.816202</td>\n",
       "      <td>1.395876e-06</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>66.209437</td>\n",
       "      <td>0.018270</td>\n",
       "      <td>-7.963360e-07</td>\n",
       "      <td>7.946199e-08</td>\n",
       "      <td>3.094850e+26</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180116e+42</td>\n",
       "      <td>3.096140e+26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.114871e+07</td>\n",
       "      <td>4909.096785</td>\n",
       "      <td>5.161176e-11</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>117.637998</td>\n",
       "      <td>1.244450e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.209994e+07</td>\n",
       "      <td>21254.775111</td>\n",
       "      <td>-1.978338e-08</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>66.206274</td>\n",
       "      <td>0.018270</td>\n",
       "      <td>-7.962482e-07</td>\n",
       "      <td>7.917201e-08</td>\n",
       "      <td>3.094850e+26</td>\n",
       "      <td>-1.552204e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180116e+42</td>\n",
       "      <td>3.096140e+26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.114871e+07</td>\n",
       "      <td>4909.096785</td>\n",
       "      <td>5.161176e-11</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>117.636902</td>\n",
       "      <td>1.204096e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.209954e+07</td>\n",
       "      <td>21430.806175</td>\n",
       "      <td>-5.367110e-07</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>66.203111</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>-7.964122e-07</td>\n",
       "      <td>7.923202e-08</td>\n",
       "      <td>3.094850e+26</td>\n",
       "      <td>-6.208817e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180116e+42</td>\n",
       "      <td>3.096140e+26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.114871e+07</td>\n",
       "      <td>4909.096785</td>\n",
       "      <td>5.161176e-11</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>117.635316</td>\n",
       "      <td>1.190659e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.209930e+07</td>\n",
       "      <td>21529.622561</td>\n",
       "      <td>-1.762539e-07</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>66.199947</td>\n",
       "      <td>0.018270</td>\n",
       "      <td>-7.964502e-07</td>\n",
       "      <td>7.947666e-08</td>\n",
       "      <td>3.094850e+26</td>\n",
       "      <td>3.104409e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.180116e+42</td>\n",
       "      <td>3.096140e+26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.114871e+07</td>\n",
       "      <td>4909.096785</td>\n",
       "      <td>5.161176e-11</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>117.633914</td>\n",
       "      <td>1.155497e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.209954e+07</td>\n",
       "      <td>21329.154532</td>\n",
       "      <td>6.907641e-07</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>66.196784</td>\n",
       "      <td>0.018270</td>\n",
       "      <td>-7.963624e-07</td>\n",
       "      <td>7.942547e-08</td>\n",
       "      <td>3.094850e+26</td>\n",
       "      <td>7.761021e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7834 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           P-PDG     P-PDG_std  P-PDG_pctchg_mean  P-PDG_pctchg_std  \\\n",
       "0   0.000000e+00  0.000000e+00                0.0               0.0   \n",
       "0   0.000000e+00  0.000000e+00                0.0               0.0   \n",
       "0   0.000000e+00  0.000000e+00                0.0               0.0   \n",
       "0   0.000000e+00  0.000000e+00                0.0               0.0   \n",
       "0   0.000000e+00  0.000000e+00                0.0               0.0   \n",
       "..           ...           ...                ...               ...   \n",
       "0  -1.180116e+42  3.096140e+26                0.0               0.0   \n",
       "0  -1.180116e+42  3.096140e+26                0.0               0.0   \n",
       "0  -1.180116e+42  3.096140e+26                0.0               0.0   \n",
       "0  -1.180116e+42  3.096140e+26                0.0               0.0   \n",
       "0  -1.180116e+42  3.096140e+26                0.0               0.0   \n",
       "\n",
       "           P-TPT     P-TPT_std  P-TPT_pctchg_mean  P-TPT_pctchg_std  \\\n",
       "0   9.994886e+06  10897.730268      -8.660628e-07          0.000007   \n",
       "0   9.994211e+06  10451.324378      -1.382307e-06          0.000007   \n",
       "0   9.993274e+06   9473.848196      -1.543777e-06          0.000006   \n",
       "0   9.992425e+06   8488.752857      -1.291574e-06          0.000006   \n",
       "0   9.991727e+06   7652.170966      -1.038462e-06          0.000006   \n",
       "..           ...           ...                ...               ...   \n",
       "0   2.114871e+07   4909.096785       5.161176e-11          0.000010   \n",
       "0   2.114871e+07   4909.096785       5.161176e-11          0.000010   \n",
       "0   2.114871e+07   4909.096785       5.161176e-11          0.000010   \n",
       "0   2.114871e+07   4909.096785       5.161176e-11          0.000010   \n",
       "0   2.114871e+07   4909.096785       5.161176e-11          0.000010   \n",
       "\n",
       "         T-TPT     T-TPT_std  ...     P-MON-CKP  P-MON-CKP_std  \\\n",
       "0   119.016257  1.142084e-02  ...  1.585979e+06  180428.771377   \n",
       "0   119.018544  6.082225e-03  ...  1.577517e+06  176934.340221   \n",
       "0   119.019854  1.916549e-03  ...  1.595649e+06  183986.562193   \n",
       "0   119.020200  1.421678e-14  ...  1.577387e+06  191753.560209   \n",
       "0   119.020200  1.421678e-14  ...  1.583204e+06  188257.219081   \n",
       "..         ...           ...  ...           ...            ...   \n",
       "0   117.638607  1.284836e-02  ...  1.209924e+07   21268.816202   \n",
       "0   117.637998  1.244450e-02  ...  1.209994e+07   21254.775111   \n",
       "0   117.636902  1.204096e-02  ...  1.209954e+07   21430.806175   \n",
       "0   117.635316  1.190659e-02  ...  1.209930e+07   21529.622561   \n",
       "0   117.633914  1.155497e-02  ...  1.209954e+07   21329.154532   \n",
       "\n",
       "    P-MON-CKP_pctchg_mean  P-MON-CKP_pctchg_std  T-JUS-CKP  T-JUS-CKP_std  \\\n",
       "0            1.753965e-04              0.014406  84.186317       0.465277   \n",
       "0            2.961627e-04              0.014392  84.181996       0.456952   \n",
       "0            2.098396e-04              0.014300  84.203006       0.478596   \n",
       "0            2.359720e-04              0.014062  84.194683       0.508876   \n",
       "0            1.326336e-04              0.013707  84.144764       0.538046   \n",
       "..                    ...                   ...        ...            ...   \n",
       "0            1.395876e-06              0.000019  66.209437       0.018270   \n",
       "0           -1.978338e-08              0.000020  66.206274       0.018270   \n",
       "0           -5.367110e-07              0.000021  66.203111       0.018271   \n",
       "0           -1.762539e-07              0.000021  66.199947       0.018270   \n",
       "0            6.907641e-07              0.000020  66.196784       0.018270   \n",
       "\n",
       "    T-JUS-CKP_pctchg_mean  T-JUS-CKP_pctchg_std         pca_1         pca_2  \n",
       "0            5.615744e-06          1.817294e-04 -1.086543e-11 -5.044664e-11  \n",
       "0           -1.779371e-06          1.674434e-04 -7.372970e-11  3.228585e-10  \n",
       "0            1.343774e-05          1.748941e-04  7.838632e-11 -4.889444e-11  \n",
       "0           -1.607210e-05          2.144586e-04 -3.725290e-11  1.984881e-10  \n",
       "0            2.366519e-06          2.196625e-04 -3.880511e-11 -3.213063e-10  \n",
       "..                    ...                   ...           ...           ...  \n",
       "0           -7.963360e-07          7.946199e-08  3.094850e+26  0.000000e+00  \n",
       "0           -7.962482e-07          7.917201e-08  3.094850e+26 -1.552204e-12  \n",
       "0           -7.964122e-07          7.923202e-08  3.094850e+26 -6.208817e-12  \n",
       "0           -7.964502e-07          7.947666e-08  3.094850e+26  3.104409e-12  \n",
       "0           -7.963624e-07          7.942547e-08  3.094850e+26  7.761021e-13  \n",
       "\n",
       "[7834 rows x 22 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['P-PDG', 'P-PDG_std','P-PDG_pctchg_mean', 'P-PDG_pctchg_std',\n",
    "            'P-TPT', 'P-TPT_std', 'P-TPT_pctchg_mean', 'P-TPT_pctchg_std',\n",
    "            'T-TPT', 'T-TPT_std', 'T-TPT_pctchg_mean', 'T-TPT_pctchg_std',\n",
    "            'P-MON-CKP', 'P-MON-CKP_std', 'P-MON-CKP_pctchg_mean', 'P-MON-CKP_pctchg_std',\n",
    "            'T-JUS-CKP', 'T-JUS-CKP_std', 'T-JUS-CKP_pctchg_mean', 'T-JUS-CKP_pctchg_std',\n",
    "            'pca_1', 'pca_2']\n",
    "X_train_pre, y_train = merged_df_train.loc[:,features], merged_df_train['class']\n",
    "X_train_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "# Pandas automatically applies colomn-wise function in the code above.\n",
    "# merged_new_df = (merged_df - merged_df.mean())/merged_df.std()\n",
    "features = ['P-PDG', 'P-PDG_std','P-PDG_pctchg_mean', 'P-PDG_pctchg_std',\n",
    "            'P-TPT', 'P-TPT_std', 'P-TPT_pctchg_mean', 'P-TPT_pctchg_std',\n",
    "            'T-TPT', 'T-TPT_std', 'T-TPT_pctchg_mean', 'T-TPT_pctchg_std',\n",
    "            'P-MON-CKP', 'P-MON-CKP_std', 'P-MON-CKP_pctchg_mean', 'P-MON-CKP_pctchg_std',\n",
    "            'T-JUS-CKP', 'T-JUS-CKP_std', 'T-JUS-CKP_pctchg_mean', 'T-JUS-CKP_pctchg_std',\n",
    "            'pca_1', 'pca_2']\n",
    "\n",
    "X_train_pre, y_train = merged_df_train.loc[:,features], merged_df_train['class']\n",
    "X_val_pre, y_val = merged_df_val.loc[:,features], merged_df_val['class']\n",
    "X_test_pre, y_test = merged_df_test.loc[:,features], merged_df_test['class']\n",
    "\n",
    "# 각 칼럼의 std는 정규화에서 제외\n",
    "columns = ['P-PDG', 'P-PDG_pctchg_mean','P-TPT','P-TPT_pctchg_mean', 'T-TPT','T-TPT_pctchg_mean', 'P-MON-CKP','P-MON-CKP_pctchg_mean', 'T-JUS-CKP','T-JUS-CKP_pctchg_mean',\n",
    "           'P-PDG_std', 'P-TPT_std', 'T-TPT_std', 'P-MON-CKP_std', 'T-JUS-CKP_std',\n",
    "           'pca_1', 'pca_2']\n",
    "\n",
    "X_train_pre.loc[:,columns] = (X_train_pre.loc[:,columns] - X_train_pre.loc[:,columns].mean())/X_train_pre.loc[:,columns].std()\n",
    "X_val_pre.loc[:,columns] = (X_val_pre.loc[:,columns] - X_val_pre.loc[:,columns].mean())/X_val_pre.loc[:,columns].std()\n",
    "X_test_pre.loc[:,columns] = (X_test_pre.loc[:,columns] - X_test_pre.loc[:,columns].mean())/X_test_pre.loc[:,columns].std()\n",
    "\n",
    "X_train = X_train_pre\n",
    "X_val = X_val_pre\n",
    "X_test = X_test_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "X_train = X_train[features].values\n",
    "y_train = y_train.values.astype(int)\n",
    "\n",
    "X_val = X_val[features].values\n",
    "y_val = y_val.values.astype(int)\n",
    "\n",
    "X_test = X_test[features].values\n",
    "y_test = y_test.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "optimizer_fn = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=0.0001)\n",
    "batch_size = 64\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW, 0.7,  0.15, 0.15, long = 1200 short = 1200, 0.95, lr = {'lr': 0.0001}, batch_size = 64, no_weigth_loss, patience = 10, val_acc = ?, test_acc = ?, no Norm, no Simul\n",
      "0 is in event, 6094\n",
      "4 is in event, 1656\n",
      "7 is in event, 84\n",
      "tensor([0.2221, 0.7886, 0.9893])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(weight_for_class)\n\u001b[1;32m     14\u001b[0m clf \u001b[39m=\u001b[39m TabNetClassifier(\n\u001b[1;32m     15\u001b[0m     optimizer_fn\u001b[39m=\u001b[39moptimizer_fn,\n\u001b[1;32m     16\u001b[0m     optimizer_params\u001b[39m=\u001b[39moptimizer_params,\u001b[39m#1e-2\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39m# cat_emb_dim = [window_size, window_size, window_size, window_size, window_size]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     )  \u001b[39m#TabNetRegressor()\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m result \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     21\u001b[0m     X_train \u001b[39m=\u001b[39;49m X_train,\n\u001b[1;32m     22\u001b[0m     y_train \u001b[39m=\u001b[39;49m y_train,\n\u001b[1;32m     23\u001b[0m     eval_set\u001b[39m=\u001b[39;49m[(X_val, y_val)],\n\u001b[1;32m     24\u001b[0m     eval_metric \u001b[39m=\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m, ],\n\u001b[1;32m     25\u001b[0m     loss_fn \u001b[39m=\u001b[39;49m nn\u001b[39m.\u001b[39;49mCrossEntropyLoss(weight_for_class),\n\u001b[1;32m     26\u001b[0m     \u001b[39m# loss_fn = nn.CrossEntropyLoss(),\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m     batch_size \u001b[39m=\u001b[39;49m batch_size,\n\u001b[1;32m     28\u001b[0m     max_epochs \u001b[39m=\u001b[39;49m max_epochs,\n\u001b[1;32m     29\u001b[0m     drop_last \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     30\u001b[0m     patience \u001b[39m=\u001b[39;49m patience\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[39m# AdamW, 0.7,  0.15, 0.15, long = 1200 short = 1200, 0.95, lr = {'lr': 0.001}, batch_size = 64, no_weigth_loss, patience = 10, val_acc = 0.96924, test_acc = 0.954, no Norm, no Simul\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# AdamW, 0.7,  0.15, 0.15, long = 1200 short = 1200, 0.95, lr = {'lr': 0.001}, batch_size = 128, no_weigth_loss, patience = 15, val_acc = .883, test_acc = .957, no Norm, no Simul\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/envs/geocon/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:217\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn \u001b[39m=\u001b[39m loss_fn\n\u001b[0;32m--> 217\u001b[0m check_input(X_train)\n\u001b[1;32m    218\u001b[0m check_warm_start(warm_start, from_unsupervised)\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_fit_params(\n\u001b[1;32m    221\u001b[0m     X_train,\n\u001b[1;32m    222\u001b[0m     y_train,\n\u001b[1;32m    223\u001b[0m     eval_set,\n\u001b[1;32m    224\u001b[0m     weights,\n\u001b[1;32m    225\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/envs/geocon/lib/python3.10/site-packages/pytorch_tabnet/utils.py:507\u001b[0m, in \u001b[0;36mcheck_input\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m    505\u001b[0m     err_message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPandas DataFrame are not supported: apply X.values when calling fit\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(err_message)\n\u001b[0;32m--> 507\u001b[0m check_array(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/envs/geocon/lib/python3.10/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[1;32m    960\u001b[0m             array,\n\u001b[1;32m    961\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    962\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    963\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    964\u001b[0m         )\n\u001b[1;32m    966\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/envs/geocon/lib/python3.10/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    125\u001b[0m     X,\n\u001b[1;32m    126\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[1;32m    127\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    128\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[1;32m    129\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    130\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/envs/geocon/lib/python3.10/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "print(f'{optimizer_fn.__name__}, {train_ratio}, {val_ratio: .2f}, {test_ratio}, long = {window_size_long} short = {window_size_short}, {overlap_ratio}, lr = {optimizer_params}, batch_size = {batch_size}, no_weigth_loss, patience = {patience}, val_acc = ?, test_acc = ?, no Norm, no Simul')\n",
    "\n",
    "weight_for_class = []\n",
    "total_obs = len(y_train)\n",
    "# check\n",
    "for key, value in sorted(Counter(y_train).items()):\n",
    "    print(f'{key} is in event, {value}')\n",
    "    weight_for_class.append(1 - (value/total_obs))\n",
    "\n",
    "max_epochs = 200\n",
    "weight_for_class = torch.Tensor(weight_for_class).type(torch.float32)\n",
    "print(weight_for_class)\n",
    "\n",
    "clf = TabNetClassifier(\n",
    "    optimizer_fn=optimizer_fn,\n",
    "    optimizer_params=optimizer_params,#1e-2\n",
    "    # cat_emb_dim = [window_size, window_size, window_size, window_size, window_size]\n",
    "    )  #TabNetRegressor()\n",
    "\n",
    "result = clf.fit(\n",
    "    X_train = X_train,\n",
    "    y_train = y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric = [\"accuracy\", ],\n",
    "    loss_fn = nn.CrossEntropyLoss(weight_for_class),\n",
    "    # loss_fn = nn.CrossEntropyLoss(),\n",
    "    batch_size = batch_size,\n",
    "    max_epochs = max_epochs,\n",
    "    drop_last = True,\n",
    "    patience = patience\n",
    ")\n",
    "# AdamW, 0.7,  0.15, 0.15, long = 1200 short = 1200, 0.95, lr = {'lr': 0.001}, batch_size = 64, no_weigth_loss, patience = 10, val_acc = 0.96924, test_acc = 0.954, no Norm, no Simul\n",
    "# AdamW, 0.7,  0.15, 0.15, long = 1200 short = 1200, 0.95, lr = {'lr': 0.001}, batch_size = 128, no_weigth_loss, patience = 15, val_acc = .883, test_acc = .957, no Norm, no Simul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9618039904154113\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     31604\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.97      0.96      0.96      1569\n",
      "           4       0.86      1.00      0.92      8919\n",
      "           5       0.00      0.00      0.00        30\n",
      "           6       0.00      0.00      0.00        10\n",
      "           7       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96     42151\n",
      "   macro avg       0.40      0.42      0.41     42151\n",
      "weighted avg       0.97      0.96      0.96     42151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "print(accuracy_score(preds, y_test))\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving_path_name = './tabnet_test_acc_9904'\n",
    "# clf.save_model(saving_path_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geocon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a4b37ad9db526a544fbe558cbc658e019a09612a6280b3580d69773eabd1425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
